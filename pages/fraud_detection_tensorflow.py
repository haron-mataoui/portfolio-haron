import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc

from imblearn.over_sampling import SMOTE
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers

# ----------------- CONFIG -----------------
st.set_page_config(
    page_title="ğŸ’³ DÃ©tection de Fraude Bancaire - TensorFlow",
    layout="wide"
)
st.title("ğŸ’³ DÃ©tection de Fraude Bancaire avec TensorFlow ")


# ----------------- DATA LOADING -----------------
@st.cache_data
def load_data():
    url = "https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv"
    df = pd.read_csv(url)
    return df

df = load_data()

# --- DÃ©finition des sous-onglets ---
tabs = st.tabs([
    " Exploration des donnÃ©es",
    " EntraÃ®nement du modÃ¨le",
    " Ã‰valuation et Visualisations",
    " PrÃ©diction en direct"
])

# ----------------- TAB 1: EXPLORATION INTERACTIVE -----------------
with tabs[0]:
    st.header("Exploration interactive du Dataset")

    st.write("""
    Ce tableau prÃ©sente un aperÃ§u des premiÃ¨res lignes du dataset.
    Chaque ligne correspond Ã  une transaction bancaire. La colonne `Class` indique si une transaction est frauduleuse (1) ou non (0).
    """)
    st.dataframe(df.head(), use_container_width=True)

    st.write("RÃ©sumÃ© statistique des colonnes numÃ©riques.")
    st.dataframe(df.describe().T, use_container_width=True)

    # Metrics clÃ©s
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Nombre total de transactions", f"{df.shape[0]:,}")
    col2.metric("Nombre de variables", df.shape[1])
    fraud_count = int(df['Class'].sum())
    fraud_percent = (fraud_count / df.shape[0]) * 100
    col3.metric("Transactions frauduleuses", f"{fraud_count} ({fraud_percent:.2f}%)")
    col4.metric("Transactions lÃ©gitimes", f"{df.shape[0] - fraud_count:,}")


    st.write("### Distribution dâ€™une colonne")
    # DÃ©finit l'index de la colonne 'Time' comme dÃ©faut
    try:
        default_ix = df.columns.get_loc('Time')
    except KeyError:
        default_ix = 0 # Fallback
    col_to_plot = st.selectbox("SÃ©lectionner une colonne Ã  visualiser", df.columns, index=default_ix)

    if pd.api.types.is_numeric_dtype(df[col_to_plot]):
        fig = px.histogram(df, x=col_to_plot, color="Class", nbins=50, log_y=True, title=f"Histogramme de {col_to_plot} par Classe")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.write("Cette colonne n'est pas numÃ©rique.")

    st.write("""
    ### Carte de corrÃ©lation
    Cette heatmap montre les relations entre les diffÃ©rentes variables. La plupart des variables (`V1` Ã  `V28`) sont le rÃ©sultat d'une Analyse en Composantes Principales (ACP) et sont donc peu corrÃ©lÃ©es entre elles.
    """)
    corr = df.corr()
    fig, ax = plt.subplots(figsize=(12, 10))
    sns.heatmap(corr, annot=False, cmap="coolwarm", ax=ax)
    st.pyplot(fig)


# ----------------- TAB 2: ENTRAÃNEMENT -----------------
with tabs[1]:
    st.header("EntraÃ®nement du ModÃ¨le")

    # --- PrÃ©paration des donnÃ©es ---
    st.subheader("1. PrÃ©paration et division des donnÃ©es")
    st.markdown("""
    La premiÃ¨re Ã©tape consiste Ã  sÃ©parer les features (`X`) de la cible (`y`).
    Ensuite, nous divisons les donnÃ©es en ensembles d'entraÃ®nement et de test.
    **C'est l'Ã©tape cruciale :** la division doit se faire *avant* toute technique de rÃ©-Ã©chantillonnage pour Ã©viter que le modÃ¨le ne "voie" des informations du jeu de test pendant son entraÃ®nement. Nous utilisons `stratify=y` pour conserver la mÃªme proportion de fraudes dans les deux ensembles.
    """)
    X = df.drop("Class", axis=1)
    y = df["Class"]

    # Division AVANT le sur-Ã©chantillonnage pour Ã©viter la fuite de donnÃ©es
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    col1, col2 = st.columns(2)
    col1.metric("Taille du jeu d'entraÃ®nement", X_train.shape[0])
    col2.metric("Taille du jeu de test", X_test.shape[0])

    # Normalisation
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test) # On applique la transformation apprise sur le train set

    # --- Gestion du dÃ©sÃ©quilibre ---
    st.subheader("2. Gestion du dÃ©sÃ©quilibre avec SMOTE")
    st.markdown("""
    Le jeu de donnÃ©es d'entraÃ®nement est **extrÃªmement dÃ©sÃ©quilibrÃ©** : il y a beaucoup plus de transactions lÃ©gitimes que de fraudes. Si on entraÃ®nait le modÃ¨le directement dessus, il deviendrait "paresseux" et prÃ©dirait presque toujours "non-fraude", obtenant un score de prÃ©cision Ã©levÃ© mais en Ã©tant inutile pour dÃ©tecter les fraudes.

    Pour corriger cela, nous utilisons **SMOTE (Synthetic Minority Over-sampling Technique)**.
    """)
    with st.expander(" Cliquez ici pour comprendre comment fonctionne SMOTE en dÃ©tail"):
        st.markdown("""
        PlutÃ´t que de simplement dupliquer les rares exemples de fraude que nous avons, SMOTE est plus intelligent :
        
        1.  Il prend un exemple de fraude au hasard.
        2.  Il trouve ses voisins les plus proches dans l'espace des features (d'autres fraudes qui lui ressemblent).
        3.  Il choisit un de ces voisins et **crÃ©e un nouvel exemple synthÃ©tique** sur la ligne qui relie les deux.
        
        Imaginez que vous avez deux points bleus (fraudes) trÃ¨s proches sur un graphique. SMOTE va ajouter un nouveau point bleu quelque part sur le segment entre ces deux points. En rÃ©pÃ©tant ce processus, on peuple l'ensemble d'entraÃ®nement avec des exemples de fraudes plausibles et variÃ©s, sans simplement copier les donnÃ©es existantes.

        **RÃ©sultat :** Le modÃ¨le dispose d'un jeu de donnÃ©es Ã©quilibrÃ© pour s'entraÃ®ner, ce qui l'oblige Ã  apprendre les caractÃ©ristiques distinctives des fraudes de maniÃ¨re beaucoup plus efficace.
        
        **Important : SMOTE est appliquÃ© UNIQUEMENT sur le jeu d'entraÃ®nement** pour ne pas introduire de donnÃ©es synthÃ©tiques dans notre ensemble de test, qui doit rester reprÃ©sentatif de la rÃ©alitÃ©.
        """)
    st.write(f"Distribution des classes **avant SMOTE** (dans le training set) :")
    st.json(pd.Series(y_train).value_counts().to_dict())

    if st.button("Lancer l'entraÃ®nement du modÃ¨le"):
        with st.spinner(" Application de SMOTE et entraÃ®nement du modÃ¨le en cours... Cela peut prendre quelques minutes."):
            smote = SMOTE(random_state=42)
            X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

            st.write(f"Distribution des classes **aprÃ¨s SMOTE** (dans le training set) :")
            st.json(pd.Series(y_train_res).value_counts().to_dict())

            # --- Construction et entraÃ®nement du modÃ¨le ---
            st.subheader("3. EntraÃ®nement du rÃ©seau de neurones")

            def build_model(input_dim):
                model = models.Sequential([
                    layers.Input(shape=(input_dim,)),
                    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
                    layers.Dropout(0.5),
                    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
                    layers.Dropout(0.3),
                    layers.Dense(1, activation='sigmoid')
                ])
                model.compile(optimizer='adam', loss='binary_crossentropy',
                              metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')])
                return model

            model = build_model(X_train_res.shape[1])

            history = model.fit(
                X_train_res, y_train_res,
                validation_data=(X_test, y_test),
                epochs=20,
                batch_size=2048,
                verbose=0
            )
            st.success(" EntraÃ®nement terminÃ© !")

            st.subheader("Architecture du modÃ¨le")
            model_summary = []
            model.summary(print_fn=lambda x: model_summary.append(x))
            st.text("\n".join(model_summary))

            # Sauvegarde dans session_state
            st.session_state.model = model
            st.session_state.scaler = scaler
            st.session_state.X_test = X_test
            st.session_state.y_test = y_test
            st.session_state.history = history
            st.session_state.df_for_prediction = df.drop("Class", axis=1)


# ----------------- TAB 3: Ã‰VALUATION -----------------
with tabs[2]:
    st.header("Ã‰valuation du ModÃ¨le sur le Jeu de Test Original")

    if "model" in st.session_state:
        model = st.session_state.model
        X_test = st.session_state.X_test
        y_test = st.session_state.y_test
        history = st.session_state.history

        # PrÃ©dictions
        y_pred_proba = model.predict(X_test)
        y_pred = (y_pred_proba > 0.5).astype(int)

        # Rapport de classification
        st.subheader("Rapport de classification")
        
        st.info("""
        ### Comment interprÃ©ter ces rÃ©sultats ?
        
        Ce tableau est le bulletin de notes le plus important pour notre modÃ¨le. L'accuracy seule est trompeuse ici. Voici comment lire les rÃ©sultats pour la classe **1 (Fraude)** :

        -   **Recall (Rappel)** : **C'est la mÃ©trique la plus cruciale.** Un rappel Ã©levÃ© signifie que le modÃ¨le a rÃ©ussi Ã  **identifier un grand pourcentage des fraudes rÃ©elles**. Notre objectif principal est d'avoir ce chiffre le plus haut possible.

        -   **Precision (PrÃ©cision)** : C'est le compromis. Une prÃ©cision faible signifie que **lorsque le modÃ¨le sonne l'alarme, il peut souvent se tromper**. Le reste du temps, ce sont des "fausses alertes".

        -   **Le Dilemme :** C'est tout Ã  fait normal ! Pour attraper un maximum de fraudes (haut rappel), le modÃ¨le doit Ãªtre trÃ¨s sensible, ce qui gÃ©nÃ¨re mÃ©caniquement plus de fausses alertes (basse prÃ©cision). Les rÃ©sultats que vous voyez sont donc **bons et rÃ©alistes** pour un systÃ¨me de dÃ©tection de fraude.
        """)
        
        report = classification_report(y_test, y_pred, output_dict=True)
        st.dataframe(pd.DataFrame(report).T.style.background_gradient(cmap='viridis', subset=['f1-score', 'precision', 'recall']))

        # Matrice de confusion
        st.subheader("Matrice de confusion")
        st.markdown("Visualise directement le nombre de bonnes et de mauvaises prÃ©dictions.")
        cm = confusion_matrix(y_test, y_pred)
        fig_cm, ax_cm = plt.subplots(figsize=(6, 5))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax_cm,
                    xticklabels=['LÃ©gitime', 'Fraude'], yticklabels=['LÃ©gitime', 'Fraude'])
        ax_cm.set_xlabel("PrÃ©diction")
        ax_cm.set_ylabel("Valeur RÃ©elle")
        ax_cm.set_title("Matrice de confusion")
        st.pyplot(fig_cm)

        # Courbes dâ€™apprentissage
        st.subheader("Courbes dâ€™apprentissage")
        fig, ax = plt.subplots(1, 2, figsize=(14, 5))
        ax[0].plot(history.history['accuracy'], label='Train Accuracy')
        ax[0].plot(history.history['val_accuracy'], label='Validation Accuracy')
        ax[0].set_title('PrÃ©cision (Accuracy)')
        ax[0].set_xlabel("Epochs")
        ax[0].set_ylabel("Accuracy")
        ax[0].legend()

        ax[1].plot(history.history['loss'], label='Train Loss')
        ax[1].plot(history.history['val_loss'], label='Validation Loss')
        ax[1].set_title('Perte (Loss)')
        ax[1].set_xlabel("Epochs")
        ax[1].set_ylabel("Loss")
        ax[1].legend()
        st.pyplot(fig)


        # Courbes ROC et Precision-Recall
        st.subheader("Courbes de performance")
        col1, col2 = st.columns(2)
        with col1:
            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            roc_auc = auc(fpr, tpr)
            fig_roc, ax_roc = plt.subplots(figsize=(6, 5))
            ax_roc.plot(fpr, tpr, label=f"AUC = {roc_auc:.3f}")
            ax_roc.plot([0, 1], [0, 1], '--', color='gray')
            ax_roc.set_title("Courbe ROC")
            ax_roc.set_xlabel("Taux de faux positifs (FPR)")
            ax_roc.set_ylabel("Taux de vrais positifs (TPR)")
            ax_roc.legend()
            st.pyplot(fig_roc)

        with col2:
            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
            pr_auc = auc(recall, precision)
            fig_pr, ax_pr = plt.subplots(figsize=(6, 5))
            ax_pr.plot(recall, precision, label=f"AUC = {pr_auc:.3f}")
            ax_pr.set_title("Courbe PrÃ©cision-Rappel")
            ax_pr.set_xlabel("Rappel (Recall)")
            ax_pr.set_ylabel("PrÃ©cision (Precision)")
            ax_pr.legend()
            st.pyplot(fig_pr)


    else:
        st.warning(" Lancez d'abord l'entraÃ®nement du modÃ¨le dans l'onglet 'EntraÃ®nement du modÃ¨le'.")


# ----------------- TAB 4: PRÃ‰DICTION -----------------
with tabs[3]:
    st.header("Tester une Transaction en Direct")


    st.info("""
    ###  Comprendre les variables `V1` Ã  `V28`

    Les colonnes `V1` Ã  `V28` ne reprÃ©sentent **pas des informations rÃ©elles** sur les transactions (comme le montant, le lieu, le type dâ€™achat, etc.).  
    Elles proviennent dâ€™une **Analyse en Composantes Principales (ACP / PCA)** rÃ©alisÃ©e pour **anonymiser les donnÃ©es dâ€™origine** tout en conservant les structures statistiques.

    ####  ConcrÃ¨tement :
    - Chaque `Vn` est une **combinaison mathÃ©matique** de plusieurs caractÃ©ristiques dâ€™origine (comme la frÃ©quence dâ€™achat, la catÃ©gorie du commerÃ§ant, lâ€™heure, etc.).
    - Ces variables ont Ã©tÃ© **transformÃ©es pour la confidentialitÃ©** : leur sens exact nâ€™est **pas interprÃ©table directement**.
    - Par exemple, `V5` ne veut pas dire â€œmontantâ€ ni â€œtype dâ€™achatâ€ : câ€™est une **dimension abstraite** du comportement transactionnel.

    ####  Pourquoi cela complique la prÃ©diction manuelle :
    Dans une application rÃ©elle de dÃ©tection de fraude, les variables `V1` Ã  `V28` seraient **calculÃ©es automatiquement** Ã  partir des donnÃ©es brutes dâ€™une transaction via la mÃªme transformation PCA que celle utilisÃ©e pour entraÃ®ner le modÃ¨le.

    Ici, comme les variables originales sont inconnues, **lâ€™utilisateur ne peut pas les saisir lui-mÃªme**.
    """)


    if "model" in st.session_state:
        model = st.session_state.model
        scaler = st.session_state.scaler
        df_cols = st.session_state.df_for_prediction

        st.write("Remplissez les champs ci-dessous pour simuler une nouvelle transaction. Les valeurs par dÃ©faut correspondent Ã  la mÃ©diane de chaque variable.")

        input_data = {}
        # CrÃ©er une grille pour les inputs
        cols = st.columns(4)
        for i, col_name in enumerate(df_cols.columns):
            with cols[i % 4]:
                 input_data[col_name] = st.number_input(f"{col_name}", value=float(df_cols[col_name].median()), key=f"input_{col_name}")


        if st.button(" Analyser la transaction"):
            input_df = pd.DataFrame([input_data])
            input_scaled = scaler.transform(input_df)
            pred_proba = model.predict(input_scaled)[0][0]

            st.subheader("RÃ©sultat de l'analyse")
            if pred_proba > 0.5:
                st.error(f"ğŸ”´ Transaction jugÃ©e **FRAUDULEUSE** avec une probabilitÃ© de {pred_proba:.2%}", icon="ğŸš¨")
            else:
                st.success(f"ğŸŸ¢ Transaction jugÃ©e **LÃ‰GITIME** avec une probabilitÃ© de {1-pred_proba:.2%}", icon="âœ…")

            st.progress(float(pred_proba))
            st.write(f"Le score de probabilitÃ© de fraude est de **{pred_proba:.4f}**. Le seuil de classification est Ã  0.5.")
    else:
        st.warning(" Lancez d'abord l'entraÃ®nement du modÃ¨le dans l'onglet 'EntraÃ®nement du modÃ¨le'.")

